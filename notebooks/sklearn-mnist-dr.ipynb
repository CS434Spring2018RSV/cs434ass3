{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST handwritten digits dimensionality reduction with scikit-learn\n",
    "\n",
    "In this notebook, we'll use some popular methods to reduce the dimensionality of MNIST digits data before classification.  \n",
    "\n",
    "First, the needed imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import decomposition, feature_selection\n",
    "from skimage.measure import block_reduce\n",
    "from skimage.feature import canny\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we load the MNIST data. First time it may download the data, which can take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "print()\n",
    "print('MNIST data loaded: train:',len(X_train),'test:',len(X_test))\n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature extraction\n",
    "\n",
    "### 1.1 PCA\n",
    "\n",
    "[Principal component analysis](http://scikit-learn.org/stable/modules/decomposition.html#pca) (PCA) is a standard method to decompose a high-dimensional dataset in a set of successive orthogonal components that explain a maximum amount of the variance. Here we project the data into `n_components` principal components. The components have the maximal possible variance under the orthogonality constraint.\n",
    "\n",
    "The option `whiten=True` can be used to whiten the outputs to have unit component-wise variances.  Its usefulness depends on the model to be used.\n",
    "\n",
    "Notice the `reshape(-1,28*28)` function which flattens the 2-D images into 1-D vectors (from 28*28 pixel images to 784-dimensional vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "n_components = 50\n",
    "pca = decomposition.PCA(n_components=n_components, whiten=True)\n",
    "X_pca = pca.fit_transform(X_train.reshape(-1,28*28))\n",
    "print('X_pca:', X_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the amount of variance explained by the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.arange(n_components)+1, pca.explained_variance_)\n",
    "plt.title('Explained variance by PCA components')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Image feature extraction\n",
    "\n",
    "There are a lot of different feature extraction methods for image data.  Common ones include extraction of colors, textures, and shapes from images, or detection of edges, corners, lines, blobs, or templates.  Let's try a simple filtering-based method to reduce the dimensionality of the features, and a widely-used edge detector.\n",
    "\n",
    "The [`measure.block_reduce()`](http://scikit-image.org/docs/dev/api/skimage.measure.html#skimage.measure.block_reduce) function from scikit-image applies a function (for_example `np.mean`, `np.max` or `np.median`) to blocks of the image, resulting in a downsampled image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filter_size = 2\n",
    "X_train_downsampled = block_reduce(X_train, \n",
    "                                   block_size=(1, filter_size, filter_size), \n",
    "                                   func=np.mean)\n",
    "print('X_train:', X_train.shape)\n",
    "print('X_train_downsampled:', X_train_downsampled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`feature.canny()`](http://scikit-image.org/docs/dev/api/skimage.feature.html#skimage.feature.canny) function applies the [Canny edge detector](https://en.wikipedia.org/wiki/Canny_edge_detector) to extract edges from the image.  Processing all images may take a couple of minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "sigma = 1.0\n",
    "X_train_canny = np.zeros(X_train.shape)\n",
    "for i in range(X_train.shape[0]):\n",
    "    X_train_canny[i,:,:] = canny(X_train[i,:,:], sigma=sigma)\n",
    "print('X_train_canny:', X_train_canny.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the original and filtered digit images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pltsize=1\n",
    "\n",
    "plt.figure(figsize=(10*pltsize, pltsize))\n",
    "plt.suptitle('Original')\n",
    "plt.subplots_adjust(top=0.8)\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(X_train[i,:,:], cmap=\"gray\", interpolation='none')\n",
    "\n",
    "plt.figure(figsize=(10*pltsize, pltsize))\n",
    "plt.suptitle('Downsampled with a %dx%d filter' % (filter_size, filter_size))\n",
    "plt.subplots_adjust(top=0.8)\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(X_train_downsampled[i,:,:], cmap=\"gray\", interpolation='none')\n",
    "    \n",
    "plt.figure(figsize=(10*pltsize, pltsize))\n",
    "plt.suptitle('Canny edge detection with sigma=%.2f' % sigma)\n",
    "plt.subplots_adjust(top=0.8)\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(X_train_canny[i,:,:], cmap=\"gray\", interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature selection\n",
    "\n",
    "### 2.1 Low variance\n",
    "\n",
    "The MNIST digits data has a lot of components with little variance.  These components are not particularly useful for discriminating between the classes, so they can probably be removed safely.  Let's first draw the component-wise variances of MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "variances = np.var(X_train.reshape(-1,28*28), axis=0)\n",
    "plt.figure()\n",
    "plt.plot(variances)\n",
    "plt.title('Component-wise variance of MNIST digits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variances can also be plotted for each pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "with sns.axes_style(\"white\"):\n",
    "    plt.imshow(variances.reshape(28,28), interpolation='none')\n",
    "plt.title('Pixel-wise variance of MNIST digits')\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select an appropriate `variance_threshold` based on the *\"Component-wise variance of MNIST digits\"* figure above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "variance_threshold = 1000\n",
    "lv = feature_selection.VarianceThreshold(threshold=variance_threshold)\n",
    "X_lv = lv.fit_transform(X_train.reshape(-1,28*28))\n",
    "print('X_lv:', X_lv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Univariate feature selection\n",
    "\n",
    "Another method for feature selection is to select the *k* best features based on univariate statistical tests between the features and the class of each sample.  Therefore, this is a supervised method and we need to include `y_train` in `fit_transform()`.\n",
    "See [scikit-learn documentation](http://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection) for the set of available statistical tests and other further options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "k = 50\n",
    "ukb = feature_selection.SelectKBest(k=k)\n",
    "X_ukb = ukb.fit_transform(X_train.reshape(-1,28*28), y_train)\n",
    "print('X_ukb:', X_ukb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check which features (that is, pixels in case) got selected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "support = ukb.get_support()\n",
    "plt.figure()\n",
    "with sns.axes_style(\"white\"):\n",
    "    plt.imshow(support.reshape(28,28), interpolation='none')\n",
    "plt.title('Support of SelectKBest() with k=%d' % k)\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Classification with dimension-reduced data \n",
    "\n",
    "Test nearest neighbor classifiers and/or decision trees with the lower-dimensional data.  Compare to classification using the original input data.\n",
    "\n",
    "Note that you need to transform the test data into the lower-dimensional space using `transform()`.  Here is an example for PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test_pca = pca.transform(X_test.reshape(-1,28*28))\n",
    "print('X_test_pca:', X_test_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Other methods for dimensionality reduction\n",
    "\n",
    "Study and experiment with additional dimensionality reduction methods based on [decomposing](http://scikit-learn.org/stable/modules/decomposition.html) or [feature selection](http://scikit-learn.org/stable/modules/feature_selection.html).  See also [unsupervised dimensionality reduction](http://scikit-learn.org/stable/modules/unsupervised_reduction.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
