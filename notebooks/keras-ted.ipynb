{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ted talks keyword labeling with pre-trained word embeddings\n",
    "\n",
    "In this notebook, we'll use pre-trained [GloVe word embeddings](http://nlp.stanford.edu/projects/glove/) for keyword labeling using Keras (version $\\ge$ 2 is required). This notebook is largely based on the blog post [Using pre-trained word embeddings in a Keras model](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html) by FranÃ§ois Chollet.\n",
    "\n",
    "**Note that using a GPU with this notebook is highly recommended.**\n",
    "\n",
    "First, the needed imports. Keras tells us which backend (Theano, Tensorflow, CNTK) it will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from distutils.version import LooseVersion as LV\n",
    "from keras import __version__\n",
    "from keras import backend as K\n",
    "\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print('Using Keras version:', __version__, 'backend:', K.backend())\n",
    "assert(LV(__version__) >= LV(\"2.0.0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe word embeddings\n",
    "\n",
    "Let's begin by loading a datafile containing pre-trained word embeddings.  The datafile contains 100-dimensional embeddings for 400,000 English words.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --content-disposition -nc https://kannu.csc.fi/s/rrCNCRdJf9LZSCE/download\n",
    "GLOVE_DIR = \"/home/cloud-user/machine-learning-scripts/notebooks\"\n",
    "\n",
    "#GLOVE_DIR = \"/home/cloud-user/glove.6B\"\n",
    "\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "print('Examples of embeddings:')\n",
    "for w in ['some', 'random', 'words']:\n",
    "    print(w, embeddings_index[w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ted talks data set\n",
    "\n",
    "Next we'll load the Ted talks data set. \n",
    "\n",
    "The dataset contains transcripts and metadata of 2085 Ted talks. Each talk is annotated with a set of keywords. In this notebook, we'll use the 10 most common keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --content-disposition -nc https://kannu.csc.fi/s/zPtriL3qqnycDFK/download\n",
    "TEXT_DATA_DIR = \"/home/cloud-user/machine-learning-scripts/notebooks\"\n",
    "\n",
    "#TEXT_DATA_DIR = \"/home/cloud-user/ted\"\n",
    "\n",
    "keywords = {\"technology\": 0, \"culture\": 1, \"science\": 2, \"global issues\": 3, \"design\": 4, \n",
    "            \"business\": 5, \"entertainment\": 6, \"arts\": 7, \"education\": 8, \"politics\": 9}\n",
    "\n",
    "print('Processing xml')\n",
    "\n",
    "tree = ET.parse(TEXT_DATA_DIR+\"/ted_en-20160408.xml\")\n",
    "root = tree.getroot() \n",
    "\n",
    "talks = []\n",
    "\n",
    "for i in root:\n",
    "    labels = np.zeros(10)\n",
    "    kws = i.find(\"./head/keywords\").text.split(\",\")\n",
    "    kws = [x.strip() for x in kws]\n",
    "    for k in kws:\n",
    "        if k in keywords:\n",
    "            labels[keywords[k]] = 1.\n",
    "    title = i.find(\"./head/title\").text\n",
    "    date = i.find(\"./head/date\").text\n",
    "    description = i.find(\"./head/description\").text\n",
    "    content = i.find(\"./content\").text\n",
    "    \n",
    "    talks.append({\"title\": title, \"date\": date, \"description\": description,\n",
    "                  \"content\": content, \"labels\": labels})\n",
    "\n",
    "print('Found %s talks.' % len(talks))\n",
    "\n",
    "nlabels_mean = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at *i*th talk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "print('* Title:', talks[i][\"title\"])\n",
    "print('* Date:', talks[i][\"date\"])\n",
    "print('* Description:', talks[i][\"description\"])\n",
    "print('* Content:', talks[i][\"content\"][:1000], '...',\n",
    "      '(%d chars in total)' % len(talks[i][\"content\"])) \n",
    "print('* Labels:', talks[i][\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we decide whether to use the `description` or `content` fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texttype = \"content\"\n",
    "#texttype = \"description\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the distribution of the length of the text fields we are using: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = np.empty(len(talks))\n",
    "for i in range(len(talks)):\n",
    "    l[i]=len(talks[i][texttype])\n",
    "plt.figure()\n",
    "plt.title('Lengths of \"%s\" fields' % texttype)\n",
    "plt.hist(l, 'auto');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, a histogram of number of labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = np.empty(len(talks))\n",
    "for i in range(len(talks)):\n",
    "    l[i]=np.sum(talks[i][\"labels\"])\n",
    "nlabels_mean = np.mean(l)\n",
    "plt.figure()\n",
    "plt.title('Number of labels, mean is %f' % nlabels_mean)\n",
    "plt.hist(l, 'auto');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize the text samples into a 2D integer tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = 10000\n",
    "MAX_SEQUENCE_LENGTH = 1000 \n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts([x[texttype] for x in talks])\n",
    "sequences = tokenizer.texts_to_sequences([x[texttype] for x in talks])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = sequence.pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = np.asarray([x['labels'] for x in talks])\n",
    "\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of labels tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into a training set and a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n",
    "print('Shape of training data tensor:', x_train.shape)\n",
    "print('Shape of training label tensor:', y_train.shape)\n",
    "print('Shape of validation data tensor:', x_val.shape)\n",
    "print('Shape of validation label tensor:', y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Preparing embedding matrix.')\n",
    "\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "embedding_dim = 100\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "print('Shape of embedding matrix:', embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-D CNN\n",
    "\n",
    "### Initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(num_words,\n",
    "                    embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=MAX_SEQUENCE_LENGTH,\n",
    "                    trainable=False))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop')\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "epochs = 20\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=16,\n",
    "          epochs=epochs, \n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(history.epoch,history.history['loss'], label='training')\n",
    "plt.plot(history.epoch,history.history['val_loss'], label='validation')\n",
    "plt.title('loss')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further analyze the results, we can produce the actual predictions for the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the correct and predicted labels for some talks in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "nb_talks_to_show = 20\n",
    "\n",
    "inv_keywords = {v: k for k, v in keywords.items()}\n",
    "for t in range(nb_talks_to_show):\n",
    "    print(t,':')\n",
    "    print('    correct: ', end='')\n",
    "    for idx in np.where(y_val[t]>0.5)[0].tolist():\n",
    "        sys.stdout.write('['+inv_keywords[idx]+'] ')\n",
    "    print()\n",
    "    print('  predicted: ', end='')\n",
    "    for idx in np.where(predictions[t]>threshold)[0].tolist():\n",
    "        sys.stdout.write('['+inv_keywords[idx]+'] ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn has some applicable performance [metrics](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) we can try: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print('Precision: {0:.3f} (threshold: {1:.2f})'\n",
    "      .format(metrics.precision_score(y_val.flatten(), predictions.flatten()>threshold), threshold))\n",
    "print('Recall: {0:.3f} (threshold: {1:.2f})'\n",
    "      .format(metrics.recall_score(y_val.flatten(), predictions.flatten()>threshold), threshold))\n",
    "print('F1 score: {0:.3f} (threshold: {1:.2f})'\n",
    "      .format(metrics.f1_score(y_val.flatten(), predictions.flatten()>threshold), threshold))\n",
    "\n",
    "average_precision = metrics.average_precision_score(y_val.flatten(), predictions.flatten())\n",
    "print('Average precision: {0:.3f}'.format(average_precision))\n",
    "print('Coverage: {0:.3f}, optimal: {1:.3f}'\n",
    "      .format(metrics.coverage_error(y_val, predictions), nlabels_mean))\n",
    "print('LRAP: {0:.3f}'\n",
    "      .format(metrics.label_ranking_average_precision_score(y_val, predictions)))\n",
    "\n",
    "precision, recall, _ = metrics.precision_recall_curve(y_val.flatten(), predictions.flatten())\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                 color='b')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('Precision-recall curve');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "\n",
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(num_words,\n",
    "                    embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=MAX_SEQUENCE_LENGTH,\n",
    "                    trainable=False))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(128))\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop')\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "epochs = 5\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=16,\n",
    "          epochs=epochs, \n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(history.epoch,history.history['loss'], label='training')\n",
    "plt.plot(history.epoch,history.history['val_loss'], label='validation')\n",
    "plt.title('loss')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "nb_talks = 10\n",
    "\n",
    "inv_keywords = {v: k for k, v in keywords.items()}\n",
    "for t in range(nb_talks):\n",
    "    print(t,':')\n",
    "    print('    correct: ', end='')\n",
    "    for idx in np.where(y_val[t]>0.5)[0].tolist():\n",
    "        sys.stdout.write('['+inv_keywords[idx]+'] ')\n",
    "    print()\n",
    "    print('  predicted: ', end='')\n",
    "    for idx in np.where(predictions[t]>threshold)[0].tolist():\n",
    "        sys.stdout.write('['+inv_keywords[idx]+'] ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import coverage_error, label_ranking_average_precision_score\n",
    "print('Coverage:', coverage_error(y_val, predictions), \", optimal:\", nlabels_mean)\n",
    "print('LRAP:', label_ranking_average_precision_score(y_val, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
