{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB movie review sentiment classification with MLPs\n",
    "\n",
    "In this notebook, we'll train a multi-layer perceptron model to classify IMDB movie reviews using **Keras** (version $\\ge$ 2 required). This notebook is largely based on the [Classifying movie reviews notebook](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/3.5-classifying-movie-reviews.ipynb) by FranÃ§ois Chollet.\n",
    "\n",
    "First, the needed imports. Keras tells us which backend (Theano, Tensorflow, CNTK) it will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "#from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "\n",
    "from distutils.version import LooseVersion as LV\n",
    "from keras import __version__\n",
    "\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print('Using Keras version:', __version__, 'backend:', K.backend())\n",
    "assert(LV(__version__) >= LV(\"2.0.0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB data set\n",
    "\n",
    "Next we'll load the IMDB dataset. First time we may have to download the data, which can take a while.\n",
    "\n",
    "The dataset has already been preprocessed, and each word has been replaced by an integer index.\n",
    "The reviews are thus represented as varying-length sequences of integers.\n",
    "(Word indices begin at \"3\", as \"1\" is used to mark the start of a review and \"2\" represents all out-of-vocabulary words.)\n",
    "\n",
    "The ground truth consists of binary sentiments for each review: positive (1) or negative (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "# number of most-frequent words \n",
    "nb_words = 10000\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=nb_words)\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "print('IMDB data loaded:')\n",
    "print('x_train:', x_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('x_test:', x_test.shape)\n",
    "print('y_test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first review in the training set: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train[0], \"length:\", len(x_train[0]), \"class:\", y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, we can convert the review back to text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in x_train[0]])\n",
    "print(decoded_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-words\n",
    "\n",
    "A MLP network expects input data to be of fixed size, so we convert the integer sequences into vectors of length `nb_words`.  Each element in the vectors corresponds to a specific word in the vocabulary: the element is \"1\" if the words appears in the review and \"0\" otherwise.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sequences(sequences, dimension=nb_words):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "# Convert training data to bag-of-words:\n",
    "X_train = vectorize_sequences(x_train)\n",
    "X_test = vectorize_sequences(x_test)\n",
    "\n",
    "# Convert labels from integers to floats:\n",
    "y_train = np.asarray(y_train).astype('float32')\n",
    "y_test = np.asarray(y_test).astype('float32')\n",
    "\n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first training review now looks like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0], \"length:\", len(X_train[0]), \"class:\", y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer perceptron (MLP) network\n",
    "\n",
    "### Initialization\n",
    "\n",
    "Let's create a three-layer MLP model that has two dense layers with *relu* activation functions.  The output layer contains a single neuron and *sigmoid* non-linearity to match the groundtruth (`y_train`). \n",
    "\n",
    "Finally, we `compile()` the model, using *binary crossentropy* as the loss function and [*RMSprop*](https://keras.io/optimizers/#rmsprop) as the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_units = 16\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=nb_units, activation='relu', input_shape=(nb_words,)))\n",
    "model.add(Dense(units=nb_units, activation='relu'))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='rmsprop', \n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning\n",
    "\n",
    "Now we are ready to train our model. An *epoch* means one pass through the whole training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "epochs = 10\n",
    "\n",
    "history = model.fit(X_train, \n",
    "                    y_train, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=512,\n",
    "                    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(history.epoch,history.history['loss'])\n",
    "plt.title('training loss')\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(history.epoch,history.history['acc'])\n",
    "plt.title('training accuracy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "For a better measure of the quality of the model, let's see the model accuracy for the test reviews. \n",
    "\n",
    "If accuracy on the test set is notably worse than with the training set, the model has likely overfitted to the training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "scores = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the learned model to predict sentiments for new reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myreviewtext = 'this movie was the worst i have ever seen and the actors were horrible'\n",
    "\n",
    "myreview = np.zeros((1,nb_words))\n",
    "myreview[0, 1] = 1.0\n",
    "\n",
    "for w in myreviewtext.split():\n",
    "    if w in word_index and word_index[w]+3<nb_words:\n",
    "            myreview[0, word_index[w]+3] = 1.0\n",
    "    else:\n",
    "        print('word not in vocabulary:', w)\n",
    "        myreview[0, 2] = 1.0\n",
    "        \n",
    "print(myreview, \"shape:\", myreview.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(myreview, batch_size=1) # values close to \"0\" mean negative, close to \"1\" positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model tuning\n",
    "\n",
    "Modify the model.  Try to improve the classification accuracy on the test set, or experiment with the effects of different parameters (e.g. number of layers, neurons on each layer, different activation functions).  \n",
    "\n",
    "You can also consult the Keras documentation at https://keras.io/.  For example, the Dense layer is documented at https://keras.io/layers/core/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
